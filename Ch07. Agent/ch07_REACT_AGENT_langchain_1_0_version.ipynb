{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ReAct(Reasoning + Acting)은 프린스턴 대학교 학자들이 구글 연구원들과 협력하여 개발한 프롬프트 엔지니어링 전략입니다.  \n",
        "\n",
        "이 전략은 LLMs(대형 언어 모델)이 인간이 현실에서 행동하는 방식처럼 먼저 문제를 어떻게 풀지 `생각`하고, `행동`을 하는 것을 흉내내는 전략입니다.  \n",
        "\n",
        "이때 ReAct는 다수의 `외부 도구`를 사용할 수 있음을 전제하며, `외부 도구`의 결과로부터 `생각`과 `행동`을 다시 진행할 수 있습니다.  \n",
        "\n",
        "이러한 상호작용은 모델이 질문을 하거나 작업을 수행하여 더 많은 정보를 얻고 상황을 더 잘 이해할 수 있도록 합니다. 예를 들어, 여러 단계를 거쳐야만 해결할 수 있는 문제가 주어져도, ReAct는 외부 도구를 호출하는 여러 검색 작업을 수행하여 문제를 해결합니다.  \n",
        "\n",
        "LLM이 `생각`과 `행동`을 번갈아 가며 수행하도록 함으로써, ReAct는 LLM을 실제 행동하는 `에이전트`로 변환하여 인간과 유사한 방식으로 작업을 완료할 수 있게 합니다."
      ],
      "metadata": {
        "id": "y4OSFoB3Z3AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReACT 에이전트는  \n",
        "\n",
        "- `생각`, `행동`, `행동 입력`, `검색 결과` 를 반복하여 문제를 해결합니다.  \n",
        "- 문제를 해결하기 위한 모든 `검색 결과`가 취합되면 마지막으로 `최종 답`을 작성합니다.  \n",
        "- `생각`: 현 시점에서 어떤 도구에 어떤 검색어를 넣어야 할 지 고민하는 단계입니다. 문제를 풀기 위해서 풀이 과정을 기재하므로 CoT 프롬프팅 전략에 해당됩니다.\n",
        "- `행동`: 실제로 어떤 도구를 사용할 지 결정하는 단계입니다.  \n",
        "- `행동 입력`: 선택한 도구에 넣을 검색어를 결정하는 단계입니다.  \n",
        "- `검색 결과`: 도구로부터 검색 결과를 얻습니다. 이 결과를 통해서 다음 `생각`이 전개됩니다."
      ],
      "metadata": {
        "id": "H1jC9xckax4y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp3T1zqxQk0Q",
        "outputId": "019880a5-1d1a-4770-96e0-8e429eedbf11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain>=1.0.0 in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
            "Collecting langchain>=1.0.0\n",
            "  Downloading langchain-1.2.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-openai>=0.2.0\n",
            "  Downloading langchain_openai-1.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-community>=0.3.0\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-text-splitters>=0.2.0\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Collecting chromadb>=0.5.5\n",
            "  Downloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting pymupdf>=1.24.0\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting sentence-transformers>=2.2.0\n",
            "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.2.1 (from langchain>=1.0.0)\n",
            "  Downloading langchain_core-1.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain>=1.0.0) (1.0.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=1.0.0) (2.12.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.2.0) (2.9.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community>=0.3.0)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.3.0) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community>=0.3.0)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.3.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.3.0) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.3.0) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community>=0.3.0)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.3.0) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.3.0) (0.4.58)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.3.0) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.3.0) (2.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0) (2025.11.3)\n",
            "Collecting build>=1.0.3 (from chromadb>=0.5.5)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb>=0.5.5)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.5) (0.38.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=0.5.5)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=0.5.5)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.5.5)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=0.5.5)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=0.5.5)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=0.5.5)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=0.5.5)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=0.5.5) (4.25.1)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0) (4.57.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0) (0.36.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.0) (1.22.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb>=0.5.5) (25.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb>=0.5.5)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.3.0)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.3.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=0.5.5) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=0.5.5) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=0.5.5) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=0.5.5) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.5.5) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=0.5.5) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=0.5.5) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=0.5.5) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.5) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.5) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.5) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.5) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.5) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb>=0.5.5)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.5.5)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain>=1.0.0) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain>=1.0.0) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=1.0.0) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=1.0.0) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=1.0.0) (0.2.15)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=1.0.0) (3.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community>=0.3.0) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community>=0.3.0) (0.25.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.5.5)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.5) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.5) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.5) (1.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.0) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.0) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.5.5) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.5) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.5)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.5)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb>=0.5.5)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb>=0.5.5)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb>=0.5.5)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.5)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=1.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=1.0.0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=1.0.0) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community>=0.3.0) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.3.0) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=0.5.5) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=0.5.5) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community>=0.3.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=0.5.5) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=0.5.5) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.5)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.5)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.5)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.5) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.5) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.5) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.5) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.5.5) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain>=1.0.0) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain>=1.0.0) (1.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.5) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.5) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.3.0)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.5)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb>=0.5.5) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.5) (0.6.1)\n",
            "Downloading langchain-1.2.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.2-py3-none-any.whl (476 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.1/476.1 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=d8a10c58dde4482b4e7ec923d29fc6e31d668d429db4d7da46dfc334775e6fdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pyproject_hooks, pymupdf, pybase64, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httptools, bcrypt, backoff, watchfiles, typing-inspect, requests, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, posthog, opentelemetry-semantic-conventions, onnxruntime, dataclasses-json, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langchain-core, sentence-transformers, langchain-text-splitters, langchain-openai, chromadb, langchain-classic, langchain-community, langchain\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.1.3\n",
            "    Uninstalling langchain-core-1.1.3:\n",
            "      Successfully uninstalled langchain-core-1.1.3\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 5.1.2\n",
            "    Uninstalling sentence-transformers-5.1.2:\n",
            "      Successfully uninstalled sentence-transformers-5.1.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.1.3\n",
            "    Uninstalling langchain-1.1.3:\n",
            "      Successfully uninstalled langchain-1.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 chromadb-1.3.7 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 langchain-1.2.0 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.2.2 langchain-openai-1.1.4 langchain-text-splitters-1.1.0 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pymupdf-1.26.7 pypika-0.48.9 pyproject_hooks-1.2.0 requests-2.32.5 sentence-transformers-5.2.0 typing-inspect-0.9.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"langchain>=1.0.0\" \"langchain-openai>=0.2.0\" \"langchain-community>=0.3.0\" \\\n",
        "               \"langchain-text-splitters>=0.2.0\" \"tiktoken>=0.7.0\" \"chromadb>=0.5.5\" \\\n",
        "               \"pymupdf>=1.24.0\" \"sentence-transformers>=2.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import requests\n",
        "\n",
        "# LangChain 1.0 계열 임포트\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.tools import create_retriever_tool\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "Dr5iHJXnQpXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"여러분의 키 값\""
      ],
      "metadata": {
        "id": "V8FHOjwJWFnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파일 다운로드"
      ],
      "metadata": {
        "id": "QPhforHBsvgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "   \"https://raw.githubusercontent.com/llama-index-tutorial/llama-index-tutorial/main/ch06/ict_japan_2024.pdf\",\n",
        "   \"https://raw.githubusercontent.com/llama-index-tutorial/llama-index-tutorial/main/ch06/ict_usa_2024.pdf\"\n",
        "]\n",
        "\n",
        "# 각 파일 다운로드\n",
        "for url in urls:\n",
        "   filename = url.split(\"/\")[-1]  # URL에서 파일명 추출\n",
        "   response = requests.get(url)\n",
        "\n",
        "   with open(filename, \"wb\") as f:\n",
        "       f.write(response.content)\n",
        "   print(f\"{filename} 다운로드 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb_tz3ZHUxRo",
        "outputId": "d3ae47c6-4c72-487a-a19e-f6434f5ec8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ict_japan_2024.pdf 다운로드 완료\n",
            "ict_usa_2024.pdf 다운로드 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github에 저장된 일본과 미국의 ICT 정책 보고서 PDF를 로컬 환경으로 가져오는 코드입니다.  \n",
        "\n",
        "urls 리스트는 Github의 raw 콘텐츠 URL을 담고 있습니다. 각 URL은 2024년 발행된 일본과 미국의 ICT 정책 보고서를 가리킵니다.  \n",
        "\n",
        "반복문은 각 URL의 파일을 차례로 다운로드합니다. split() 메서드로 URL의 마지막 부분에서 파일명을 추출하고, requests로 원격 파일의 내용을 받아옵니다. 받아온 내용은 바이너리 쓰기 모드로 로컬에 저장되며, 저장이 끝날 때마다 완료 메시지가 출력됩니다. 이 파일들은 벡터 데이터베이스 구축을 위한 원본 자료로 사용될 예정입니다."
      ],
      "metadata": {
        "id": "9OkGkNOA24C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 임베딩 설정"
      ],
      "metadata": {
        "id": "NGpDiTEe2_Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 설정\n",
        "embd = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "a0dKRZ__26dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF 로드 및 벡터 DB"
      ],
      "metadata": {
        "id": "rGMJKQVxsxE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF 파일은 아래 파일을 인터넷에 검색하면 나오는데 그걸 업로드하세요"
      ],
      "metadata": {
        "id": "eGttj3kweemj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pdf_retriever(\n",
        "    pdf_path: str,  # PDF 파일 경로\n",
        "    persist_directory: str,  # 벡터 스토어 저장 경로\n",
        "    embedding_model: OpenAIEmbeddings,  # OpenAIEmbeddings 임베딩 모델\n",
        "    chunk_size: int = 512,  # 청크 크기 기본값 512\n",
        "    chunk_overlap: int = 0  # 청크 오버랩 크기 기본값 0\n",
        ") -> Chroma.as_retriever:\n",
        "\n",
        "    # PDF 파일로드. 페이지 별로 로드.\n",
        "    loader = PyMuPDFLoader(pdf_path)\n",
        "    data = loader.load()\n",
        "\n",
        "    # 청킹. 길이를 주면, 해당 길이가 넘지 않도록 자르는 것.\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=chunk_size, # PDF의 각 페이지를 최대 길이 512가 넘지 않도록 잘게 분할.\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(data)\n",
        "\n",
        "    # 벡터 DB에 적재\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        persist_directory=persist_directory,\n",
        "        documents=doc_splits,\n",
        "        embedding=embedding_model,\n",
        "    )\n",
        "\n",
        "    # 벡터 DB를 retriever 객체로 반환.\n",
        "    return vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "1RiOIzYmUyXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create_pdf_retriever 함수는 PDF 문서를 벡터 데이터베이스로 변환하고 이를 검색할 수 있는 retriever를 생성하는 함수입니다. 이 함수는 PDF 파일 경로, 벡터 저장소 경로, OpenAI 임베딩 모델을 필수 입력으로 받으며, 청크 크기와 오버랩 크기는 선택적 매개변수로 기본값이 각각 512와 0으로 설정되어 있습니다.  \n",
        "\n",
        "함수 내부적으로 PDF를 처리하는 과정은 세 단계로 이루어집니다. 먼저 PyMuPDF 라이브러리로 PDF를 읽어들입니다. 그 다음 RecursiveCharacterTextSplitter를 사용해 텍스트를 청크 단위로 분할합니다. 이는 긴 문서를 다수의 문서로 분할하기 위함입니다. 분할된 다수의 문서들은 OpenAI의 임베딩 모델을 통해 Chroma 벡터 데이터베이스에 저장됩니다. 마지막으로 create_pdf_retriever 함수는 생성된 벡터 데이터베이스의 검색 인터페이스(retriever)를 반환합니다. 이 retriever는 자연어 질의에 대해 관련성 높은 PDF 내용을 검색할 수 있는 검색기입니다.  \n",
        "\n",
        "이제 위 함수를 이용하여 일본 ICT 정책에 대해서 검색하는 검색기와 미국 ICT 정책에 대해서 검색하는 검색기를 각각 만들어봅시다."
      ],
      "metadata": {
        "id": "SE-ObPLO3HYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 일본 ICT 정책 데이터베이스 생성\n",
        "retriever_japan = create_pdf_retriever(\n",
        "    pdf_path=\"ict_japan_2024.pdf\",\n",
        "    persist_directory=\"db_ict_policy_japan_2024\",\n",
        "    embedding_model=embd\n",
        ")\n",
        "\n",
        "# 미국 ICT 정책 데이터베이스 생성\n",
        "retriever_usa = create_pdf_retriever(\n",
        "    pdf_path=\"ict_usa_2024.pdf\",\n",
        "    persist_directory=\"db_ict_policy_usa_2024\",\n",
        "    embedding_model=embd\n",
        ")"
      ],
      "metadata": {
        "id": "9USrv-pd3Rq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create_pdf_retriever 함수를 사용해 일본과 미국의 ICT 정책 문서를 각각의 벡터 데이터베이스 검색기로 생성합니다. persist_directory에 \"db_ict_policy_japan_2024\"와 \"db_ict_policy_usa_2024\"라는 서로 다른 경로를 지정함으로써, 두 문서의 벡터 데이터베이스가 서로 영향을 주지 않고 독립적으로 저장되고 관리됩니다. 만약 같은 경로를 사용했다면 두 번째 문서 처리 시 첫 번째 문서의 데이터가 덮어써지거나 혼합될 수 있기 때문에, 이렇게 경로를 분리하는 것이 중요합니다.  \n",
        "\n",
        "일본 ICT 정책의 경우 \"ict_japan_2024.pdf\" 파일을 읽어 \"db_ict_policy_japan_2024\" 디렉토리에 벡터 데이터베이스를 생성합니다. 마찬가지로 미국 ICT 정책은 \"ict_usa_2024.pdf\" 파일을 \"db_ict_policy_usa_2024\" 디렉토리에 저장합니다.  \n",
        "\n",
        "이렇게 두 개의 검색기를 만들었습니다. 이제 이렇게 만들어진 두 개의 retriever 객체를 create_retriever_tool 함수를 통해 ReAct 에이전트가 사용할 수 있는 검색 도구로 변환해봅시다."
      ],
      "metadata": {
        "id": "l8ZhDm7N3S3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jp_engine = create_retriever_tool(\n",
        "    retriever=retriever_japan,\n",
        "    name=\"japan_ict\",\n",
        "    description=\"일본의 ICT 시장동향 정보를 제공합니다. 일본 ICT와 관련된 질문은 해당 도구를 사용하세요.\",\n",
        ")\n",
        "\n",
        "usa_engine = create_retriever_tool(\n",
        "    retriever=retriever_usa,\n",
        "    name=\"usa_ict\",\n",
        "    description=\"미국의 ICT 시장동향 정보를 제공합니다. 미국 ICT와 관련된 질문은 해당 도구를 사용하세요.\",\n",
        ")"
      ],
      "metadata": {
        "id": "QrXhC_Y3U8ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [jp_engine, usa_engine]\n",
        "tool_map: Dict[str, object] = {t.name: t for t in tools}"
      ],
      "metadata": {
        "id": "8J20uylG3Vxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create_retriever_tool 함수는 retriever 객체를 ReACT 에이전트가 활용할 수 있는 형태의 도구로 변환해주는 함수입니다. 앞서 생성한 retriever_japan과 retriever_usa 객체를 바탕으로 도구의 이름(name)과 설명(description)을 추가하여 ReAct 에이전트가 활용할 수 있는 형태로 만듭니다.  \n",
        "\n",
        "description에는 각 검색기의 상세한 용도를 작성해야 합니다. 예를 들어 일본 ICT 검색기의 경우 \"일본의 ICT 시장동향 정보를 제공합니다. 일본 ICT와 관련된 질문은 해당 도구를 사용하세요\"라는 설명을 작성했습니다. 여기서 주의할 점은 각 도구에 대한 description을 매우 상세하게 작성해야 한다는 점입니다. 이후 ReAct 에이전트가 동작할 때, 에이전트는 여기서 적힌 설명을 보고 사용자의 질문에 따라서 도구를 선택합니다. 따라서 description에 도구에 대한 설명이 제대로 적혀있지 않다면, 에이전트는 상황에 맞는 도구 선택을 제대로 할 수 없으므로 주의합니다.\n",
        "\n",
        "생성된 jp_engine과 usa_engine은 tools 리스트에 담았습니다. 이 tools 리스트는 뒤의 코드에서 ReAct 에이전트에게 전달됩니다. 에이전트는 이 도구들의 상세한 description을 통해 각 질문에 가장 적합한 도구를 선택하고 효과적인 검색을 수행할 수 있게 됩니다."
      ],
      "metadata": {
        "id": "puaTuu5l3aNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct 프롬프트"
      ],
      "metadata": {
        "id": "Inp3AjWrs0GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 프롬프트 템플릿\n",
        "# =========================\n",
        "react_template = '''다음 질문에 최선을 다해 답변하세요. 당신은 다음 도구들에 접근할 수 있습니다:\n",
        "\n",
        "{tools}\n",
        "\n",
        "다음 형식을 사용하세요:\n",
        "\n",
        "Question: 답변해야 하는 입력 질문\n",
        "Thought: 무엇을 할지 항상 생각하세요\n",
        "Action: 취해야 할 행동, [{tool_names}] 중 하나여야 합니다. 리스트에 있는 도구 중 1개를 택하십시오.\n",
        "Action Input: 행동에 대한 입력값\n",
        "Observation: 행동의 결과\n",
        "... (이 Thought/Action/Action Input/Observation의 과정이 N번 반복될 수 있습니다)\n",
        "Thought: 이제 최종 답변을 알겠습니다\n",
        "Final Answer: 원래 입력된 질문에 대한 최종 답변 (한글로 작성하십시오.)\n",
        "\n",
        "## 추가적인 주의사항\n",
        "- 반드시 [Thought -> Action -> Action Input -> Observation] 순서를 준수하십시오. 항상 Action 전에는 Thought가 먼저 나와야 합니다.\n",
        "- 최종 답변에는 최대한 많은 내용을 포함하십시오.\n",
        "- 한 번의 검색으로 해결되지 않을 것 같다면 문제를 분할하여 푸는 것도 고려하십시오.\n",
        "- 정보가 취합되었다면 불필요하게 사이클을 반복하지 마십시오.\n",
        "- 묻지 않은 정보를 찾으려고 도구를 사용하지 마십시오.\n",
        "\n",
        "시작하세요!\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}'''\n",
        "\n",
        "prompt = PromptTemplate.from_template(react_template)"
      ],
      "metadata": {
        "id": "Ky8bSwSCV74G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제로는 가장 처음 GPT-4.1을 호출할 때는 아래와 같이 프롬프트가 완성될 것입니다.  \n",
        "\n",
        "```\n",
        "다음 질문에 최선을 다해 답변하세요. 당신은 다음 도구들에 접근할 수 있습니다:\n",
        "\n",
        "japan_ict: 일본의 ICT 시장동향 정보를 제공합니다. 일본 ICT와 관련된 질문은 해당 도구를 사용하세요.\n",
        "usa_ict: 미국의 ICT 시장동향 정보를 제공합니다. 미국 ICT와 관련된 질문은 해당 도구를 사용하세요.\n",
        "\n",
        "다음 형식을 사용하세요:\n",
        "\n",
        "Question: 답변해야 하는 입력 질문\n",
        "Thought: 무엇을 할지 항상 생각하세요\n",
        "Action: 취해야 할 행동, [japan_ict, usa_ict] 중 하나여야 합니다. 리스트에 있는 도구 중 1개를 택하십시오.\n",
        "Action Input: 행동에 대한 입력값\n",
        "Observation: 행동의 결과\n",
        "... (이 Thought/Action/Action Input/Observation의 과정이 N번 반복될 수 있습니다)\n",
        "Thought: 이제 최종 답변을 알겠습니다\n",
        "Final Answer: 원래 입력된 질문에 대한 최종 답변 (한글로 작성하십시오.)\n",
        "\n",
        "## 추가적인 주의사항\n",
        "- 반드시 [Thought -> Action -> Action Input -> Observation] 순서를 준수하십시오. 항상 Action 전에는 Thought가 먼저 나와야 합니다.\n",
        "- 최종 답변에는 최대한 많은 내용을 포함하십시오.\n",
        "- 한 번의 검색으로 해결되지 않을 것 같다면 문제를 분할하여 푸는 것도 고려하십시오.\n",
        "- 정보가 취합되었다면 불필요하게 사이클을 반복하지 마십시오.\n",
        "- 묻지 않은 정보를 찾으려고 도구를 사용하지 마십시오.\n",
        "\n",
        "시작하세요!\n",
        "\n",
        "Question: 세계 인공지능 시장규모를 년도 별로 설명해줘\n",
        "```"
      ],
      "metadata": {
        "id": "E6y3L4wOQida"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 프롬프트 구성 함수"
      ],
      "metadata": {
        "id": "XwDeg9KIskwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 프롬프트 구성을 위한 함수\n",
        "# =========================\n",
        "def _format_tools_for_prompt(ts: List[object]) -> Tuple[str, str]:\n",
        "    lines, names = [], []\n",
        "    for t in ts:\n",
        "        names.append(t.name)\n",
        "        desc = getattr(t, \"description\", \"\")\n",
        "        lines.append(f\"{t.name}: {desc}\")\n",
        "    return \"\\n\".join(lines), \", \".join(names)\n",
        "\n",
        "def _render_prompt(user_input: str, scratchpad: str) -> str:\n",
        "    tools_str, tool_names = _format_tools_for_prompt(tools)\n",
        "    return prompt.format(\n",
        "        tools=tools_str,\n",
        "        tool_names=tool_names,\n",
        "        input=user_input,\n",
        "        agent_scratchpad=scratchpad,\n",
        "    )"
      ],
      "metadata": {
        "id": "JRIpD3NFqscz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`_format_tools_for_prompt` 함수\n",
        "\n",
        "이 함수는 도구 목록을 프롬프트에 삽입할 수 있는 문자열 형태로 변환하는 용도로 사용됩니다.\n",
        "\n",
        "예를 들어 `japan_ict`라는 도구가 있고 설명이 `\"일본의 ICT 시장동향 정보를 제공합니다. 일본 ICT와 관련된 질문은 해당 도구를 사용하세요.\"`라면, 이 함수를 거치면 첫 번째 반환값으로 `\"일본의 ICT 시장동향 정보를 제공합니다. 일본 ICT와 관련된 질문은 해당 도구를 사용하세요.\"`라는 문자열이 나오고, 두 번째 반환값으로 `\"japan_ict\"`라는 이름만 나옵니다.\n",
        "\n",
        "도구가 여러 개라면 첫 번째 반환값은 줄바꿈(`\\\\n`)으로 연결되고, 두 번째 반환값은 쉼표(`, `)로 연결됩니다. 이렇게 두 가지를 분리해서 반환하는 이유는 프롬프트 템플릿에서 `{tools}` 자리에는 도구 설명 전체가, `{tool_names}` 자리에는 이름 목록만 들어가야 하기 때문입니다.\n",
        "\n",
        "`_render_prompt` 함수\n",
        "\n",
        "이 함수는 LLM에 실제로 전달할 최종 프롬프트 문자열을 조립하는 용도로 사용됩니다.\n",
        "\n",
        "예를 들어 사용자가 `\"세계 인공지능 시장규모를 년도 별로 설명해줘\"`라고 질문했다면, `user_input`에 이 문장이 들어가고 `prompt.format()`을 통해 템플릿의 `{input}` 자리에 채워집니다.\n",
        "\n",
        "`scratchpad` 파라미터는 ReAct 루프가 반복될 때마다 이전 단계의 Thought, Action, Observation 기록이 누적된 문자열입니다. 예를 들어 첫 번째 반복에서 LLM이 도구를 호출하고 결과를 받았다면, 두 번째 반복에서는 그 기록이 `scratchpad`에 담겨서 `{agent_scratchpad}` 자리에 삽입됩니다. 이렇게 해야 LLM이 이전에 무슨 행동을 했고 어떤 결과를 받았는지 맥락을 유지하면서 다음 단계를 결정할 수 있습니다."
      ],
      "metadata": {
        "id": "YlKGGw1TrViU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# LLM 설정\n",
        "# =========================\n",
        "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)"
      ],
      "metadata": {
        "id": "dtCey6-R7O_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정규표현식 패턴과 파싱 함수\n"
      ],
      "metadata": {
        "id": "BZGx5GWYsGvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ReAct 파서 및 실행 루프\n",
        "# =========================\n",
        "ACTION_RE = re.compile(r\"^Action\\s*:\\s*(?P<tool>.+?)\\s*$\", re.MULTILINE)\n",
        "ACTION_INPUT_RE = re.compile(r\"^Action Input\\s*:\\s*(?P<input>.+?)\\s*$\", re.MULTILINE)\n",
        "FINAL_ANSWER_RE = re.compile(r\"Final Answer\\s*:\\s*(?P<final>[\\s\\S]+)$\", re.IGNORECASE)"
      ],
      "metadata": {
        "id": "A7LUc5Hs58MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ACTION_RE`, `ACTION_INPUT_RE`, `FINAL_ANSWER_RE`는 LLM 응답에서 특정 패턴을 추출하는 용도로 사용됩니다.\n",
        "\n",
        "예를 들어 LLM이 아래와 같이 응답했다면:\n",
        "\n",
        "```\n",
        "Thought: 일본 ICT 산업 동향을 검색해야겠다\n",
        "Action: japan_ict\n",
        "Action Input: 인공지능 시장규모\n",
        "```\n",
        "\n",
        "`ACTION_RE`는 `Action:` 뒤의 `ICT_industry_search`를 추출하고, `ACTION_INPUT_RE`는 `Action Input:` 뒤의 `인공지능 시장규모`를 추출합니다. `FINAL_ANSWER_RE`는 LLM이 `Final Answer:`로 최종 답변을 제시했을 때 그 내용을 추출합니다."
      ],
      "metadata": {
        "id": "tcb5KapNsECW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse_action_and_input(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "    m_final = FINAL_ANSWER_RE.search(text)\n",
        "    if m_final:\n",
        "        return \"__FINAL__\", m_final.group(\"final\").strip()\n",
        "    m_act = ACTION_RE.search(text)\n",
        "    m_in = ACTION_INPUT_RE.search(text)\n",
        "    if m_act and m_in:\n",
        "        return m_act.group(\"tool\").strip(), m_in.group(\"input\").strip()\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "UWWusz2iqynm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수는 LLM 응답 텍스트를 파싱해서 다음에 취할 행동을 판별하는 용도로 사용됩니다.\n",
        "\n",
        "예를 들어 LLM 응답에 `Final Answer: 인공지능 시장은...`이 포함되어 있다면, `(\"__FINAL__\", \"인공지능 시장은...\")`을 반환합니다. `\"__FINAL__\"`은 루프를 종료하라는 신호로 사용됩니다.\n",
        "\n",
        "만약 `Final Answer`가 없고 `Action: ICT_industry_search`와 `Action Input: 시장규모`가 있다면, `(\"ICT_industry_search\", \"시장규모\")`를 반환합니다. 둘 다 찾지 못하면 `(None, None)`을 반환해서 파싱 실패를 알립니다."
      ],
      "metadata": {
        "id": "_m4I70YasNFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실행 결과 처리"
      ],
      "metadata": {
        "id": "L-83RBj1sVCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _observation_to_text(observation_obj) -> str:\n",
        "    if isinstance(observation_obj, list):\n",
        "        def doc_to_str(d):\n",
        "            try:\n",
        "                meta = getattr(d, \"metadata\", {}) or {}\n",
        "                src = meta.get(\"source\") or meta.get(\"file_path\") or \"\"\n",
        "                txt = getattr(d, \"page_content\", \"\")\n",
        "                if len(txt) > 500:\n",
        "                    txt = txt[:500] + \"...\"\n",
        "                return f\"[source={src}] {txt}\"\n",
        "            except Exception:\n",
        "                return str(d)\n",
        "        return \"\\n\".join(doc_to_str(d) for d in observation_obj[:5])\n",
        "    return str(observation_obj)"
      ],
      "metadata": {
        "id": "4dZs_vYNrxYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수는 도구 실행 결과를 문자열로 변환하는 용도로 사용됩니다.\n",
        "\n",
        "예를 들어 `create_retriever_tool`로 만든 검색 도구는 `Document` 객체 리스트를 반환합니다. 이 함수는 각 `Document`에서 `page_content`와 `metadata`의 `source` 정보를 추출해서 `\"[source=파일경로] 문서내용...\"`와 같은 형태로 변환합니다. 문서 내용이 500자를 넘으면 잘라내고 `...`를 붙입니다. 최대 5개 문서까지만 처리하는데, 너무 많은 내용이 `scratchpad`에 쌓이면 컨텍스트 윈도우를 낭비하기 때문입니다."
      ],
      "metadata": {
        "id": "RjEZQPqqsUYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실제 실행부"
      ],
      "metadata": {
        "id": "H90i9lkFsaXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_react(user_input: str, max_iters: int = 8) -> Dict[str, str]:\n",
        "    scratchpad = \"\"\n",
        "    for _ in range(max_iters):\n",
        "        rendered = _render_prompt(user_input, scratchpad)\n",
        "        resp = llm.invoke(rendered)\n",
        "        text = resp.content if hasattr(resp, \"content\") else str(resp)\n",
        "\n",
        "        tool, action_input = _parse_action_and_input(text)\n",
        "        if tool is None:\n",
        "            hint = \"\\n[파싱안내] 형식을 엄격히 따르세요. 반드시 'Action:'와 'Action Input:'을 한 줄씩 제공하십시오.\\n\"\n",
        "            scratchpad += f\"{text}\\n{hint}\"\n",
        "            continue\n",
        "\n",
        "        if tool == \"__FINAL__\":\n",
        "            final_answer = action_input\n",
        "            return {\"output\": final_answer, \"log\": scratchpad + \"\\n\" + text}\n",
        "\n",
        "        if tool not in tool_map:\n",
        "            observation = f\"[에러] 존재하지 않는 도구입니다: {tool}\"\n",
        "            scratchpad += f\"{text}\\nObservation: {observation}\\n\"\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            observation_obj = tool_map[tool].invoke(action_input)\n",
        "            observation = _observation_to_text(observation_obj)\n",
        "            scratchpad += f\"{text}\\nObservation: {observation}\\n\"\n",
        "        except Exception as e:\n",
        "            scratchpad += f\"{text}\\nObservation: [도구실행오류] {e}\\n\"\n",
        "\n",
        "    return {\n",
        "        \"output\": \"반복 한도를 초과했습니다. 질문을 더 구체화해 주세요.\",\n",
        "        \"log\": scratchpad,\n",
        "    }"
      ],
      "metadata": {
        "id": "MS4GPSCEr0Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수는 ReAct 에이전트의 메인 실행 루프로, Thought → Action → Observation 사이클을 반복하는 용도로 사용됩니다.\n",
        "\n",
        "예를 들어 사용자가 `\"세계 인공지능 시장규모를 년도 별로 설명해줘\"`라고 질문하면, 다음과 같은 흐름으로 진행됩니다:\n",
        "\n",
        "1. `_render_prompt`로 프롬프트를 조립하고 `llm.invoke`로 LLM을 호출합니다\n",
        "2. `_parse_action_and_input`으로 응답을 파싱합니다\n",
        "3. 파싱 실패 시 힌트 메시지를 `scratchpad`에 추가하고 다시 시도합니다\n",
        "4. `\"__FINAL__\"`이면 최종 답변을 반환하고 루프를 종료합니다\n",
        "5. 도구 이름이 `tool_map`에 없으면 에러 메시지를 `scratchpad`에 추가합니다\n",
        "6. 도구가 유효하면 `tool_map[tool].invoke(action_input)`으로 실행하고, 결과를 `_observation_to_text`로 변환해서 `scratchpad`에 `Observation:`으로 추가합니다\n",
        "\n",
        "`scratchpad`는 매 반복마다 누적되어 다음 LLM 호출 시 `{agent_scratchpad}` 자리에 들어갑니다. 이렇게 해야 LLM이 이전에 어떤 도구를 호출했고 어떤 결과를 받았는지 기억하면서 다음 단계를 결정할 수 있습니다.\n",
        "\n",
        "`max_iters`는 무한 루프 방지용으로, 기본값 8회 안에 `Final Answer`가 나오지 않으면 반복 한도 초과 메시지를 반환합니다."
      ],
      "metadata": {
        "id": "V1M_IeMssdL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 테스트"
      ],
      "metadata": {
        "id": "Neh5TkoYsd6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단일 질문\n",
        "result = run_react(\"한국과 미국의 ICT 기관 협력 사례\")\n",
        "print(\"최종 답변:\", result[\"output\"])\n",
        "print(\"\\n=== 실행 로그 ===\\n\")\n",
        "print(result[\"log\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxNzlOGcq0M3",
        "outputId": "6d79cf56-c688-4169-e05f-e05a555891d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 답변: 한국과 미국의 ICT 기관 협력 사례는 다음과 같습니다.\n",
            "\n",
            "1. 한미 ICT 정책 포럼: 양국 정부는 정기적으로 ‘한미 ICT 정책 포럼’을 개최하여 정보통신 정책, 기술 동향, 표준화, 사이버보안 등 다양한 분야에서 협력 방안을 논의하고 있습니다.\n",
            "\n",
            "2. 5G/6G 차세대 이동통신 공동연구: 한국의 과학기술정보통신부(MSIT)와 미국 상무부(DoC), 연방통신위원회(FCC) 등 주요 ICT 기관은 5G와 6G 등 차세대 이동통신 기술 개발을 위해 공동 연구와 표준화 협력을 추진하고 있습니다.\n",
            "\n",
            "3. 한미 ICT 협력 양해각서(MOU): 2022년 양국은 디지털 전환, 네트워크 인프라, 디지털 포용, 표준화 등 다양한 분야에서 협력 강화를 위한 MOU를 체결하였습니다.\n",
            "\n",
            "4. 사이버보안 및 AI 협력: 인공지능(AI), 데이터 경제, 사이버보안 등 첨단 ICT 분야에서 정책 교류와 공동 대응 체계를 구축하고 있습니다.\n",
            "\n",
            "5. 연구기관 및 민간기업 협력: 한국 ETRI와 미국 NIST 등 연구기관, 그리고 양국의 주요 ICT 기업들이 5G/6G, AI, 반도체, 클라우드 등 첨단 기술 분야에서 공동 연구와 기술 교류를 활발히 진행하고 있습니다.\n",
            "\n",
            "이처럼 한국과 미국은 정부, 연구기관, 민간기업 등 다양한 주체가 ICT 분야에서 긴밀하게 협력하고 있습니다.\n",
            "\n",
            "=== 실행 로그 ===\n",
            "\n",
            "\n",
            "Thought: 미국의 ICT 시장동향 정보에서 한미 ICT 기관 협력 사례에 대한 정보를 찾아야 합니다.\n",
            "Action: usa_ict\n",
            "Action Input: 한국과 미국의 ICT 기관 협력 사례\n",
            "Observation: \n",
            "- 한미 ICT 협력은 주로 정보통신기술(ICT) 정책, 연구개발, 표준화, 사이버보안, 5G/6G 등 첨단기술 분야에서 활발히 이루어지고 있습니다.\n",
            "- 대표적인 협력 사례로는 2021년 한미 정상회담 이후 양국 정부 간 ‘한미 ICT 정책 포럼’ 정례 개최, 5G·6G 차세대 이동통신 기술 공동연구, 인공지능(AI) 및 데이터 경제 협력, 사이버보안 공동 대응 등이 있습니다.\n",
            "- 한국의 과학기술정보통신부(MSIT)와 미국 상무부(DoC), 연방통신위원회(FCC) 등 주요 ICT 기관 간 정책 교류 및 공동 프로젝트가 진행되고 있습니다.\n",
            "- 2022년에는 한미 양국이 ‘한미 ICT 협력 양해각서(MOU)’를 체결하여, 디지털 전환, 네트워크 인프라, 디지털 포용, 표준화 등 다양한 분야에서 협력 체계를 강화하였습니다.\n",
            "- 양국 연구기관(ETRI, NIST 등) 및 민간기업 간에도 5G/6G, AI, 반도체, 클라우드 등 첨단 ICT 분야에서 공동 연구 및 기술 교류가 활발히 이루어지고 있습니다.\n",
            "\n",
            "Thought: 미국 측 정보를 확보했으니, 일본 ICT 도구는 사용할 필요가 없습니다. 이제 최종 답변을 정리할 수 있습니다.\n",
            "Final Answer: 한국과 미국의 ICT 기관 협력 사례는 다음과 같습니다.\n",
            "\n",
            "1. 한미 ICT 정책 포럼: 양국 정부는 정기적으로 ‘한미 ICT 정책 포럼’을 개최하여 정보통신 정책, 기술 동향, 표준화, 사이버보안 등 다양한 분야에서 협력 방안을 논의하고 있습니다.\n",
            "\n",
            "2. 5G/6G 차세대 이동통신 공동연구: 한국의 과학기술정보통신부(MSIT)와 미국 상무부(DoC), 연방통신위원회(FCC) 등 주요 ICT 기관은 5G와 6G 등 차세대 이동통신 기술 개발을 위해 공동 연구와 표준화 협력을 추진하고 있습니다.\n",
            "\n",
            "3. 한미 ICT 협력 양해각서(MOU): 2022년 양국은 디지털 전환, 네트워크 인프라, 디지털 포용, 표준화 등 다양한 분야에서 협력 강화를 위한 MOU를 체결하였습니다.\n",
            "\n",
            "4. 사이버보안 및 AI 협력: 인공지능(AI), 데이터 경제, 사이버보안 등 첨단 ICT 분야에서 정책 교류와 공동 대응 체계를 구축하고 있습니다.\n",
            "\n",
            "5. 연구기관 및 민간기업 협력: 한국 ETRI와 미국 NIST 등 연구기관, 그리고 양국의 주요 ICT 기업들이 5G/6G, AI, 반도체, 클라우드 등 첨단 기술 분야에서 공동 연구와 기술 교류를 활발히 진행하고 있습니다.\n",
            "\n",
            "이처럼 한국과 미국은 정부, 연구기관, 민간기업 등 다양한 주체가 ICT 분야에서 긴밀하게 협력하고 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 질문은 \"한국과 미국의 ICT 기관 협력 사례\"였으며, 사용 가능한 도구는 retriever_japan과 retriever_usa입니다.\n",
        "\n",
        "ReAct 에이전트가 시작되자 에이전트는 질문의 의도에 따라서 적절한 도구를 배정하기 위해서 Thought(생각 과정)을 거칩니다. 생각 과정을 통해 질문이 미국의 ICT 시장과 연관이 있으므로, Action 단계에서 retriever_usa 도구를 선택하고, Action Input 단계에서 \"한국과 미국의 ICT 기관 협력 사례\"라는 검색어를 사용하게 됩니다.\n",
        "\n",
        "이에 대한 관찰 결과(Observation)를 얻고난 후에, 거대 언어 모델은 \"관찰 결과를 통해 한국과 미국의 ICT 기관 협력 사례에 대한 정보를 얻었습니다. 이제 이 정보를 바탕으로 최종 답변을 구성할 수 있습니다.\"라는 생각을 통해서 최종 답변(Final Answer)를 작성하게 됩니다."
      ],
      "metadata": {
        "id": "_fHhQvRN6BOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 멀티 쿼리\n",
        "result = run_react(\"미국과 일본의 ICT 주요 정책의 공통점과 차이점을 설명해줘.\", max_iters=10)\n",
        "print(\"최종 답변:\", result[\"output\"])\n",
        "print(\"\\n=== 실행 로그 ===\\n\")\n",
        "print(result[\"log\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6M7cKyLXJ9H",
        "outputId": "045b84a4-43ea-49b7-d794-00b1f1c9c965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 답변: 미국과 일본의 ICT 주요 정책은 다음과 같은 공통점과 차이점이 있습니다.\n",
            "\n",
            "공통점:\n",
            "1. 디지털 인프라 확충과 5G/차세대 네트워크 구축을 중점적으로 추진합니다.\n",
            "2. 사이버보안 강화, 인공지능(AI) 및 데이터 활용 촉진, 디지털 포용(디지털 격차 해소), 개인정보 보호 및 데이터 거버넌스, 혁신 생태계 조성 등 유사한 정책 목표를 가지고 있습니다.\n",
            "3. 반도체, 클라우드 컴퓨팅, IoT, 양자기술 등 첨단 ICT 분야의 경쟁력 강화에 집중하고 있습니다.\n",
            "\n",
            "차이점:\n",
            "1. 정책 추진 방식에서 미국은 민간 주도의 혁신과 규제 완화, 글로벌 기술 리더십 확보에 중점을 두는 반면, 일본은 정부 주도의 정책 추진과 민관 협력, 규제 개선에 더 무게를 두고 있습니다.\n",
            "2. 일본은 스마트시티, 지역 활성화, 그린 ICT(탄소중립) 등 사회 전반의 디지털 전환과 지속가능성에 대한 정책이 상대적으로 강조됩니다.\n",
            "3. 미국은 첨단기술 공급망 강화, 특히 반도체 등 전략산업의 글로벌 리더십 확보에 더 적극적이며, 혁신 생태계의 자유로운 경쟁 환경 조성에 초점을 맞춥니다.\n",
            "\n",
            "요약하면, 양국 모두 첨단 ICT 기술과 인프라, 보안, 데이터 활용에 집중하지만, 미국은 민간 주도와 글로벌 리더십, 일본은 정부 주도와 사회 전반의 디지털 전환 및 지속가능성에 더 중점을 두는 차이가 있습니다.\n",
            "\n",
            "=== 실행 로그 ===\n",
            "\n",
            "\n",
            "Thought: 미국과 일본의 ICT 주요 정책의 공통점과 차이점을 파악하기 위해 양국의 ICT 정책 동향을 각각 조사해야 합니다. 먼저 미국의 ICT 정책 동향을 확인하겠습니다.\n",
            "Action: usa_ict\n",
            "Action Input: 미국의 ICT 주요 정책 동향\n",
            "Observation: 미국의 ICT 주요 정책은 디지털 인프라 확충, 5G 및 차세대 네트워크 구축, 사이버보안 강화, 인공지능(AI) 및 데이터 경제 활성화, 디지털 포용(디지털 격차 해소), 개인정보 보호 및 규제, 혁신 생태계 조성 등이 중심입니다. 최근에는 반도체 및 첨단기술 공급망 강화, 클라우드 컴퓨팅, 양자컴퓨팅, IoT, 에너지 효율화 등도 중점적으로 추진되고 있습니다. 또한, 민간 주도의 혁신과 규제 완화, 글로벌 기술 리더십 확보가 중요한 정책 방향입니다.\n",
            "\n",
            "Thought: 이제 일본의 ICT 주요 정책 동향을 확인하겠습니다.\n",
            "Action: japan_ict\n",
            "Action Input: 일본의 ICT 주요 정책 동향\n",
            "Observation: 일본의 ICT 주요 정책은 디지털 전환(DX) 가속화, 5G 및 차세대 네트워크 인프라 구축, 사이버보안 강화, 인공지능(AI) 및 데이터 활용 촉진, 디지털 포용(디지털 격차 해소), 개인정보 보호 및 데이터 거버넌스, 스마트시티 및 지역 활성화, 혁신 생태계 조성 등이 있습니다. 최근에는 반도체 및 첨단기술 자립, 클라우드 컴퓨팅, IoT, 양자기술, 그린 ICT(탄소중립) 등도 중점적으로 추진되고 있습니다. 정부 주도의 정책 추진과 민관 협력, 규제 개선이 중요한 특징입니다.\n",
            "\n",
            "Thought: 미국과 일본의 ICT 정책에서 공통점과 차이점을 정리할 수 있습니다.\n",
            "Final Answer: 미국과 일본의 ICT 주요 정책은 다음과 같은 공통점과 차이점이 있습니다.\n",
            "\n",
            "공통점:\n",
            "1. 디지털 인프라 확충과 5G/차세대 네트워크 구축을 중점적으로 추진합니다.\n",
            "2. 사이버보안 강화, 인공지능(AI) 및 데이터 활용 촉진, 디지털 포용(디지털 격차 해소), 개인정보 보호 및 데이터 거버넌스, 혁신 생태계 조성 등 유사한 정책 목표를 가지고 있습니다.\n",
            "3. 반도체, 클라우드 컴퓨팅, IoT, 양자기술 등 첨단 ICT 분야의 경쟁력 강화에 집중하고 있습니다.\n",
            "\n",
            "차이점:\n",
            "1. 정책 추진 방식에서 미국은 민간 주도의 혁신과 규제 완화, 글로벌 기술 리더십 확보에 중점을 두는 반면, 일본은 정부 주도의 정책 추진과 민관 협력, 규제 개선에 더 무게를 두고 있습니다.\n",
            "2. 일본은 스마트시티, 지역 활성화, 그린 ICT(탄소중립) 등 사회 전반의 디지털 전환과 지속가능성에 대한 정책이 상대적으로 강조됩니다.\n",
            "3. 미국은 첨단기술 공급망 강화, 특히 반도체 등 전략산업의 글로벌 리더십 확보에 더 적극적이며, 혁신 생태계의 자유로운 경쟁 환경 조성에 초점을 맞춥니다.\n",
            "\n",
            "요약하면, 양국 모두 첨단 ICT 기술과 인프라, 보안, 데이터 활용에 집중하지만, 미국은 민간 주도와 글로벌 리더십, 일본은 정부 주도와 사회 전반의 디지털 전환 및 지속가능성에 더 중점을 두는 차이가 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 질문은 \"미국과 일본의 ICT 주요 정책의 공통점과 차이점\"이었으며, 사용 가능한 도구는 retriever_japan과 retriever_usa입니다. 이는 하나의 질문이지만 두 국가의 정보를 필요로 하는 복합적인 성격을 띠고 있습니다.  \n",
        "\n",
        "ReAct 에이전트가 시작되자 에이전트는 질문의 의도에 따라서 적절한 도구를 배정하기 위해서 Thought(생각 과정)을 거칩니다. 생각 과정을 통해 두 국가의 정책을 비교하기 위해서는 각각의 정보가 필요하다고 판단하여, 하나의 질문을 두 단계로 나누어 접근합니다. 먼저 Action 단계에서 retriever_usa 도구를 선택하고 Action Input 단계에서 \"미국의 ICT 주요 정책 동향\"이라는 검색어를 사용합니다.  \n",
        "\n",
        "미국의 정책에 대한 관찰 결과(Observation)를 얻고 난 후, 에이전트는 다시 생각 과정을 통해 일본의 정책 정보도 필요하다고 판단합니다. 이에 따라 두 번째 Action으로 japan_ict 도구를 선택하고 \"일본의 ICT 주요 정책 동향책\"을 검색합니다. 이처럼 에이전트는 하나의 복합적인 질문을 해결하기 위해 가용한 도구들을 순차적으로 활용하는 전략을 취합니다.  \n",
        "\n",
        "두 나라의 정책 정보를 모두 수집한 후, 에이전트는 \"미국과 일본의 ICT 정책에서 공통점과 차이점을 정리할 수 있습니다.\"라는 생각을 거쳐 공통점과 차이점을 체계적으로 정리한 최종 답변(Final Answer)을 작성하게 됩니다."
      ],
      "metadata": {
        "id": "CHpPU5RT6HwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = run_react(\"미국의 ICT 관련 정부 기구, 주요 법령, 국내 기업 진출 사례 각각 따로 검색해. 그렇게 해서 정보 좀 모아봐. 그리고 나서 일본의 AI 정책도 알려줘.\", max_iters=10)\n",
        "print(\"최종 답변:\", result[\"output\"])\n",
        "print(\"\\n=== 실행 로그 ===\\n\")\n",
        "print(result[\"log\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Syhu5oSZZcn",
        "outputId": "a138417e-26a4-4fe4-b798-3eb14b44855d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 답변: 미국의 ICT 관련 정부 기구, 주요 법령, 국내 기업 진출 사례, 그리고 일본의 AI 정책에 대해 다음과 같이 정리할 수 있습니다.\n",
            "\n",
            "1. 미국의 ICT 관련 정부 기구  \n",
            "- 연방통신위원회(FCC): 통신 및 방송 정책 총괄  \n",
            "- 국가통신정보관리청(NTIA, 상무부 산하): 인터넷 정책, 스펙트럼 관리  \n",
            "- 국토안보부(DHS): 사이버보안 등 ICT 보안  \n",
            "- 에너지부(DOE): 에너지 ICT  \n",
            "- 국방부(DOD): 국방 ICT\n",
            "\n",
            "2. 미국의 ICT 관련 주요 법령  \n",
            "- 통신법(Telecommunications Act): 통신산업 전반 규제  \n",
            "- 클라우드법(CLOUD Act): 해외 데이터 접근 및 관리  \n",
            "- 사이버보안정보공유법(CISA): 사이버보안 정보 공유  \n",
            "- 개인정보보호법(CCPA, 캘리포니아): 개인정보 보호  \n",
            "- 아동온라인프라이버시보호법(COPPA): 아동 개인정보 보호  \n",
            "- 디지털밀레니엄저작권법(DMCA): 디지털 저작권 보호\n",
            "\n",
            "3. 미국 ICT 시장 내 국내(한국) 기업 진출 사례  \n",
            "- 삼성전자: 스마트폰, 반도체, 5G 등에서 미국 시장 점유율 2위  \n",
            "- LG전자: 가전, 스마트폰 등에서 강세  \n",
            "- SK하이닉스: 반도체 공급  \n",
            "- 네이버: 클라우드, AI, 현지 기업과 협력  \n",
            "- 카카오: 모빌리티, 콘텐츠, 현지 서비스  \n",
            "- KT, SKT: 5G, 통신장비 등 ICT 인프라 사업\n",
            "\n",
            "4. 일본의 AI 정책  \n",
            "- 일본 정부는 AI를 국가 성장의 핵심으로 보고, 'AI 전략 2019', 'AI 전략 2022' 등을 발표  \n",
            "- 주요 정책 방향: AI 인재 양성, 산업별 AI 활용 촉진, 데이터 인프라 구축, 윤리·신뢰성 확보, 국제 협력  \n",
            "- AI 거버넌스 체계 마련, 공공 데이터 개방, AI 연구개발 투자 확대, AI 윤리 가이드라인 제정  \n",
            "- 2023년부터는 생성형 AI(Generative AI) 정책 논의도 본격화\n",
            "\n",
            "이상으로 요청하신 정보를 종합하여 안내드립니다.\n",
            "\n",
            "=== 실행 로그 ===\n",
            "\n",
            "\n",
            "Thought: 먼저 미국의 ICT 관련 정부 기구에 대해 조사해야 합니다.\n",
            "Action: usa_ict\n",
            "Action Input: 미국 ICT 관련 정부 기구 정보\n",
            "Observation: 미국의 ICT 관련 주요 정부 기구로는 연방통신위원회(FCC), 상무부 산하 국가통신정보관리청(NTIA), 국토안보부(DHS), 에너지부(DOE), 국방부(DOD) 등이 있습니다. FCC는 통신 및 방송 정책을 총괄하며, NTIA는 인터넷 정책, 스펙트럼 관리 등을 담당합니다. DHS는 사이버보안, DOE는 에너지 ICT, DOD는 국방 ICT를 관장합니다.\n",
            "\n",
            "Thought: 이제 미국의 ICT 관련 주요 법령을 조사해야 합니다.\n",
            "Action: usa_ict\n",
            "Action Input: 미국 ICT 관련 주요 법령 정보\n",
            "Observation: 미국의 ICT 관련 주요 법령으로는 통신법(Telecommunications Act), 클라우드법(CLOUD Act), 사이버보안정보공유법(CISA), 개인정보보호법(CCPA, 캘리포니아), 아동온라인프라이버시보호법(COPPA), 디지털밀레니엄저작권법(DMCA) 등이 있습니다. 이 법령들은 통신, 데이터 보호, 저작권, 사이버보안 등 다양한 ICT 분야를 포괄합니다.\n",
            "\n",
            "Thought: 이제 미국 ICT 시장에서의 국내(한국) 기업 진출 사례를 조사해야 합니다.\n",
            "Action: usa_ict\n",
            "Action Input: 미국 ICT 시장 내 한국 기업 진출 사례\n",
            "Observation: 미국 ICT 시장에 진출한 한국 기업으로는 삼성전자(스마트폰, 반도체, 5G), LG전자(가전, 스마트폰), SK하이닉스(반도체), 네이버(클라우드, AI), 카카오(모빌리티, 콘텐츠), KT·SKT(5G, 통신장비) 등이 있습니다. 삼성전자는 미국 내 스마트폰 시장 점유율 2위, LG전자는 가전 시장에서 강세를 보이고 있습니다. 네이버와 카카오는 AI, 클라우드, 콘텐츠 분야에서 현지 기업과 협력하거나 직접 서비스를 제공하고 있습니다.\n",
            "\n",
            "Thought: 이제 일본의 AI 정책에 대해 조사해야 합니다.\n",
            "Action: japan_ict\n",
            "Action Input: 일본 AI 정책 정보\n",
            "Observation: 일본 정부는 AI 전략을 국가 성장의 핵심으로 삼고 있습니다. 2019년 'AI 전략 2019'를 발표했으며, 2022년에는 'AI 전략 2022'로 업데이트했습니다. 주요 내용은 AI 인재 양성, 산업별 AI 활용 촉진, 데이터 활용 인프라 구축, 윤리·신뢰성 확보, 국제 협력 강화 등입니다. 일본은 AI 거버넌스 체계 마련, 공공 데이터 개방, AI 연구개발 투자 확대, AI 윤리 가이드라인 제정 등을 추진하고 있습니다. 또한, 2023년에는 생성형 AI(Generative AI) 정책 논의도 본격화하고 있습니다.\n",
            "\n",
            "Thought: 이제 모든 정보를 취합하여 최종 답변을 작성할 수 있습니다.\n",
            "Final Answer: \n",
            "\n",
            "미국의 ICT 관련 정부 기구, 주요 법령, 국내 기업 진출 사례, 그리고 일본의 AI 정책에 대해 다음과 같이 정리할 수 있습니다.\n",
            "\n",
            "1. 미국의 ICT 관련 정부 기구  \n",
            "- 연방통신위원회(FCC): 통신 및 방송 정책 총괄  \n",
            "- 국가통신정보관리청(NTIA, 상무부 산하): 인터넷 정책, 스펙트럼 관리  \n",
            "- 국토안보부(DHS): 사이버보안 등 ICT 보안  \n",
            "- 에너지부(DOE): 에너지 ICT  \n",
            "- 국방부(DOD): 국방 ICT\n",
            "\n",
            "2. 미국의 ICT 관련 주요 법령  \n",
            "- 통신법(Telecommunications Act): 통신산업 전반 규제  \n",
            "- 클라우드법(CLOUD Act): 해외 데이터 접근 및 관리  \n",
            "- 사이버보안정보공유법(CISA): 사이버보안 정보 공유  \n",
            "- 개인정보보호법(CCPA, 캘리포니아): 개인정보 보호  \n",
            "- 아동온라인프라이버시보호법(COPPA): 아동 개인정보 보호  \n",
            "- 디지털밀레니엄저작권법(DMCA): 디지털 저작권 보호\n",
            "\n",
            "3. 미국 ICT 시장 내 국내(한국) 기업 진출 사례  \n",
            "- 삼성전자: 스마트폰, 반도체, 5G 등에서 미국 시장 점유율 2위  \n",
            "- LG전자: 가전, 스마트폰 등에서 강세  \n",
            "- SK하이닉스: 반도체 공급  \n",
            "- 네이버: 클라우드, AI, 현지 기업과 협력  \n",
            "- 카카오: 모빌리티, 콘텐츠, 현지 서비스  \n",
            "- KT, SKT: 5G, 통신장비 등 ICT 인프라 사업\n",
            "\n",
            "4. 일본의 AI 정책  \n",
            "- 일본 정부는 AI를 국가 성장의 핵심으로 보고, 'AI 전략 2019', 'AI 전략 2022' 등을 발표  \n",
            "- 주요 정책 방향: AI 인재 양성, 산업별 AI 활용 촉진, 데이터 인프라 구축, 윤리·신뢰성 확보, 국제 협력  \n",
            "- AI 거버넌스 체계 마련, 공공 데이터 개방, AI 연구개발 투자 확대, AI 윤리 가이드라인 제정  \n",
            "- 2023년부터는 생성형 AI(Generative AI) 정책 논의도 본격화\n",
            "\n",
            "이상으로 요청하신 정보를 종합하여 안내드립니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 질문은 \"미국의 ICT 관련 정부 기구, 주요 법령, 국내 기업 진출 사례, 그리고 일본의 AI 정책\"입니다. 질문이 명확하게 네 가지 세부 항목으로 나뉘어 있어서 각각에 대한 개별 검색이 필요한 상황입니다.  \n",
        "\n",
        "ReAct 에이전트가 시작되자 에이전트는 질문의 의도에 따라서 적절한 도구를 배정하기 위해서 Thought(생각 과정)을 거칩니다. 에이전트는 미국 관련 세 가지 정보를 먼저 retriever_usa로 순차적으로 검색합니다: \"미국의 ICT 관련 정부 기구\", \"미국의 ICT 관련 주요 법령\", \"미국의 ICT 관련 국내 기업 진출 사례\". 각 검색마다 관찰 결과(Observation)를 확인하고 다음 검색으로 넘어갑니다.  \n",
        "\n",
        "미국 관련 정보 수집을 마친 후, japan_ict 도구를 사용해 \"일본의 AI 정책\"을 검색합니다. 모든 정보가 수집되면 에이전트는 \"이제 최종 답변을 제공할 수 있습니다\"라는 생각을 거쳐 체계적인 최종 답변(Final Answer)을 작성합니다."
      ],
      "metadata": {
        "id": "ZszkjAWM6mrf"
      }
    }
  ]
}